# RTGS AI Analyst â€” Real-Time Government Statistics AI Pipeline

ğŸ“Š **A fully data-agnostic, AI-powered pipeline that transforms raw datasets into policy-ready insights and reports.**

This system automatically ingests, cleans, standardizes, and analyzes structured data to produce **executive summaries, technical analysis, and government-ready policy reports** â€” all in a single command.


<img width="2050" height="3840" alt="Image" src="https://github.com/user-attachments/assets/29020b75-34ad-4de6-99e7-a1ae404f06db" />


## âœ¨ Features

* **Data Agnostic**: Works with *any CSV/Excel dataset* without custom coding.
* **Domain-Aware**: Auto-detects relevant context (transport, health, urban, education, etc.).
* **Interactive Mode**: Optionally asks contextual questions (skippable).
* **LLM-Powered Agents**: Schema inference, narrative insights, polished reporting, memory, and observability.
* **Modular Architecture**: Built on **LangGraph** + **LangChain** for flexible, agentic orchestration.
* **Full Data Lifecycle**: Raw â†’ Standardized â†’ Cleaned â†’ Transformed â†’ Analyzed â†’ Reported.
* **Rich Deliverables**: Markdown summaries, PDF reports, technical analysis dashboards, and policy recommendations.

---

## ğŸš€ Quickstart

### 1. Clone Repository

```bash
git clone https://github.com/Nithin8919/RTGS-Style_AI_Analyst.git
cd RTGS-Style_AI_Analyst
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Configure Environment

Copy the environment template and update if needed:

```bash
cp env_template.txt .env
give your api key
```

### 4. Run Analysis

Basic run with your dataset:

```bash
python cli.py run --dataset path/to/your/dataset.csv
```

Interactive mode (asks contextual questions) optional to enter :

```bash
python cli.py run --dataset data/raw/sample.csv --interactive
```

Auto-detect domain:

```bash
python cli.py run --dataset data/raw/sample.csv --domain auto
```

---

## ğŸ”„ Data Flow

```
Raw Data â†’ Ingestion â†’ Schema Inference â†’ Standardization â†’ Cleaning 
â†’ Transformation â†’ Validation â†’ Analysis â†’ Insights â†’ Report Assembly
```

Artifacts are saved systematically:

* `data/raw/` â†’ Original files
* `data/standardized/` â†’ Normalized schemas
* `data/cleaned/` â†’ Clean datasets
* `data/transformed/` â†’ Analysis-ready features
* `artifacts/reports/` â†’ Markdown + PDF reports
* `artifacts/logs/` â†’ Execution logs & traces
* `memory/store/` â†’ Run history & reusable schema mappings

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ artifacts
â”‚   â”œâ”€â”€ docs/                # Documentation artifacts
â”‚   â”œâ”€â”€ logs/                # Execution logs
â”‚   â””â”€â”€ reports/             # Generated reports (Markdown, PDF, dashboards)
â”œâ”€â”€ backend_server.py         # Backend server entry point
â”œâ”€â”€ cli.py                    # CLI runner for datasets
â”œâ”€â”€ config.yaml               # Configurations
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw/                 # Input datasets
â”‚   â”œâ”€â”€ standardized/        # Schema-normalized datasets
â”‚   â”œâ”€â”€ cleaned/             # Clean datasets
â”‚   â””â”€â”€ transformed/         # Analysis-ready datasets
â”œâ”€â”€ docs/                     # Developer documentation
â”œâ”€â”€ env_template.txt          # Environment template
â”œâ”€â”€ Frontend/                 # Frontend (React + Tailwind + Vite)
â”‚   â”œâ”€â”€ src/                  # Components, services, UI logic
â”‚   â””â”€â”€ vite.config.ts        # Build config
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ store/               # Memory DB + run history
â”œâ”€â”€ README.md                 # Project overview
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # Core agents
â”‚   â”‚   â”œâ”€â”€ ingestion_agent.py
â”‚   â”‚   â”œâ”€â”€ schema_inference_agent.py
â”‚   â”‚   â”œâ”€â”€ standardization_agent.py
â”‚   â”‚   â”œâ”€â”€ cleaning_agent.py
â”‚   â”‚   â”œâ”€â”€ transformation_agent.py
â”‚   â”‚   â”œâ”€â”€ validator_agent.py
â”‚   â”‚   â”œâ”€â”€ analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ insight_agent.py
â”‚   â”‚   â”œâ”€â”€ report_agent.py
â”‚   â”‚   â”œâ”€â”€ memory_agent.py
â”‚   â”‚   â””â”€â”€ observability_agent.py
â”‚   â”œâ”€â”€ orchestrator/        # Flow control + agent routing
â”‚   â””â”€â”€ utils/               # Helpers, logging, visualization
â””â”€â”€ uploads/                  # User-uploaded datasets
```

---

## ğŸ§© Agent Architecture

### Data Preparation Agents

1. ğŸ“¥ **Ingestion Agent** â€“ fetches, validates, fingerprints raw data.
2. ğŸ” **Schema Inference Agent (LLM)** â€“ AI-powered column typing, naming, confidence scoring.
3. ğŸ—ï¸ **Standardization Agent** â€“ consistent formats, snake\_case, unit normalization.
4. ğŸ§¹ **Cleaning Agent** â€“ handles missing data, duplicates, outliers.
5. âš¡ **Transformation Agent** â€“ builds derived features, per-capita metrics, domain-specific KPIs.

### Analysis & Reporting Agents

6. âœ… **Validator Agent** â€“ quality gates, data confidence scoring.
7. ğŸ“Š **Analysis Agent** â€“ stats, trends, correlations, hypothesis tests.
8. ğŸ’¡ **Insight Generation Agent (LLM)** â€“ narratives, recommendations, impact statements.
9. ğŸ“‹ **Report Assembly Agent (LLM)** â€“ publication-ready Markdown + PDF.

### Infrastructure Agents

10. ğŸ§  **Memory Agent (LLM)** â€“ schema reuse, fingerprinting, caching.
11. ğŸ‘ï¸ **Observability Agent (LLM)** â€“ traces, logs, performance monitoring.

---

## ğŸ—ï¸ Under the Hood

* **LangGraph** â€“ orchestrates multi-agent workflow as a state graph.
* **LangChain** â€“ powers LLM interactions and tool execution.
* **Advanced Statistical Suite** â€“ robust descriptive, inferential, and correlation analysis.
* **Memory Layer** â€“ SQLite + JSON for schema mappings, run history, and LLM caching.

---

## ğŸ® Usage Guide

### Basic Commands

#### **Full Analysis**
```bash
python cli.py run --dataset "path/to/your/dataset.csv"
```

#### **Interactive Mode** (Ask contextual questions)
```bash
python cli.py run --dataset "data/health_indicators.xlsx" --interactive
```

#### **Domain-Specific Analysis**
```bash
python cli.py run --dataset "data/transport_data.csv" --domain transport
```

#### **Custom Scope**
```bash
python cli.py run --dataset "data/education_survey.csv" --scope "Telangana 2020"
```

### CLI Options

- `--dataset`: Path to your dataset (CSV/Excel)
- `--domain`: Domain hint (transport, health, education, urban, auto)
- `--scope`: Geographic/temporal scope (e.g., "Telangana 2020", "Hyderabad district")
- `--interactive`: Ask domain-specific questions for context
- `--output-summary`: Generate only summary (no full report)
- `--export-pdf`: Generate PDF report

### Command Examples

```bash
# Run full pipeline
python cli.py run --dataset data/raw/education.csv --scope "Telangana 2020"

```



### Report Structure
Every run generates four key outputs inside `artifacts/reports/`:

* **Executive Summary** â€“ One-page policy briefing.
* **Technical Analysis** â€“ Statistical details, quality dashboards, correlation matrices.
* **Policy Report (PDF)** â€“ Action items, interventions, budget estimates, success metrics.
* **Logs & Run History** â€“ Traceable execution + schema memory.

**Example Output Structure:**
```
artifacts/reports/rtgs-education-20250905-001/
â”œâ”€â”€ ğŸ“‹ enhanced_policy_insights.md           # Run metadata & parameters
â”œâ”€â”€ ğŸ“ enhanced_techinal_analysis.md                  # Full markdown report  
â”œâ”€â”€ ğŸ“„ techincal_anlysisr.pdf    
â”œâ”€â”€ ğŸ“„ enhanced_policy_insights(Graphical DATA).pdf              # PDF version

â”œâ”€â”€ ğŸ“ data/                       # Processed datasets
â”‚   â”œâ”€â”€ raw_sample.csv             # Original data sample
â”‚   â”œâ”€â”€ standardized.csv           # Post-standardization  
â”‚   â”œâ”€â”€ cleaned.csv                # Post-cleaning
â”‚   â””â”€â”€ transformed.csv            # Analysis-ready final dataset
â””â”€â”€ ğŸ“‹ logs/                       # Audit trail
    â”œâ”€â”€ transform_log.jsonl        # Every transform with justification

```

---



### **Frontend Integration**
The project includes a React frontend for non-technical users which is not completely ready yet it has some placeholder analysis and visualization:

```bash
cd Frontend
npm install
npm run dev
# Access at http://localhost:3000
```

---


## ğŸŒ Scope & Use Cases

Designed for **policy makers, analysts, and data teams** across:

* **Urban Development**: Infrastructure planning, service distribution analysis
* **Health & Education**: Resource allocation, performance monitoring  
* **Transportation**: Traffic analysis, infrastructure optimization
* **Economic Analysis**: Budget allocation, development indicators
* **Evidence-Based Governance**: Data-driven policy making

---

## ğŸš€ Why RTGS Stands Out

### **Hackathon Edge Factors**

1. **ğŸ“Š Data Agnostic**: Works with ANY government dataset structure
2. **ğŸ¤– Multi-Agent Intelligence**: 11 specialized AI agents working together
3. **ğŸ“‹ Policy-First**: Every output is policy-actionable with confidence scores
4. **ğŸ” Full Transparency**: Complete audit trail of every transformation
5. **âš¡ Speed**: Minutes vs weeks for analysis
6. **ğŸ—ï¸ Production Ready**: LangGraph orchestration, proper error handling
7. **ğŸ¯ Government Focus**: Built specifically for policy makers

### **Technical Depth**
- **LangGraph**: State-of-the-art agent orchestration
- **LangChain**: Modular LLM tool integration  
- **Statistical Rigor**: Hypothesis testing, confidence intervals, effect sizes
- **Memory System**: Learning from past runs for efficiency

### **Policy Impact**
- **Executive Summaries**: One-page policy briefs
- **Actionable Recommendations**: Prioritized by impact and confidence
- **Evidence-Based**: Every recommendation backed by statistical evidence
- **Risk Assessment**: Confidence scoring for decision support

---

## ğŸ¤ Contributing

Contributions welcome! Please fork, branch, and open a PR. For major changes, discuss via issues first.



---

ğŸ”¥ **Run once. Get policy-ready insights. Anywhere, any dataset.**
